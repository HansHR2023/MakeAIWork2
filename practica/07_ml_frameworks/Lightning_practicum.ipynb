{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86037b88-38ed-4373-9995-65deda9dbe37",
   "metadata": {},
   "source": [
    "# Practicum Lightning\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/starter/introduction.html#define-a-lightningmodule <br>\n",
    "Lightning in 15 minutes<br>\n",
    "PyTorch Lightning is the deep learning framework with “batteries included” for professional AI researchers and machine learning engineers who need maximal flexibility while super-charging performance at scale.<br>\n",
    "<br>\n",
    "Lightning organizes PyTorch code to remove boilerplate and unlock scalability.<br>\n",
    "\n",
    "## 1: Install PyTorch Lightning\n",
    "\n",
    "For pip users: pip install lightning<br>\n",
    "\n",
    "## 2: Define a LightningModule\n",
    "<br>\n",
    "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d048b1-5a90-4086-af54-94eefcc84ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03746ddb-4b82-4d57-b684-2ea55c002264",
   "metadata": {},
   "source": [
    "## 3: Define a dataset\n",
    "\n",
    "Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaf6a14-9be7-4add-92b9-b25213c78599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 10192076.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/train-images-idx3-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 28881/28881 [00:00<00:00, 7945929.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/train-labels-idx1-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 5772054.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 3672037.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2fcdb-7b2c-42c3-a2d8-97f47be8af5d",
   "metadata": {},
   "source": [
    "## 4: Train the model\n",
    "\n",
    "The Lightning Trainer “mixes” any LightningModule with any dataset and abstracts away all the engineering complexity needed for scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec29a734-5acf-47a8-909b-20b523b0d6bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium' | 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16866f64-eb36-4783-8634-df6f998a99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/pans/workspace/MakeAIWork2/practica/07_ml_frameworks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/pans/miniconda3/envs/miw/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9c0b8b09b34b9bb0c8c51127cf79f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454248-5e26-4dd9-93c5-2f0d86607967",
   "metadata": {},
   "source": [
    "GPU (Graphical Processing Unit) available: True (cuda), used: True\n",
    "TPU (Tensor PU) available: False, using: 0 TPU cores\n",
    "IPU (Intelligence PU) available: False, using: 0 IPUs\n",
    "HPU (Havana Gaudi PU )available: False, using: 0 HPUs\n",
    "/home/pien/miniconda3/envs/miw/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
    "  warning_cache.warn(\n",
    "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "Missing logger folder: /home/pien/Workspace/MakeAIWork2/practica/Practicum Lightning/lightning_logs\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
    "\n",
    "  | Name    | Type       | Params\n",
    "---------------------------------------\n",
    "0 | encoder | Sequential | 50.4 K\n",
    "1 | decoder | Sequential | 51.2 K\n",
    "---------------------------------------\n",
    "101 K     Trainable params\n",
    "0         Non-trainable params\n",
    "101 K     Total params\n",
    "0.407     Total estimated model params size (MB)\n",
    "/home/pien/miniconda3/envs/miw/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
    "  rank_zero_warn(\n",
    "\n",
    "Training: 0it [00:00, ?it/s]\n",
    "\n",
    "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba44a-4988-4d95-8db0-60becb1288a9",
   "metadata": {},
   "source": [
    "### The Lightning Trainer automates 40+ tricks including:\n",
    "\n",
    "- Epoch and batch iteration\n",
    "- optimizer.step(), loss.backward(), optimizer.zero_grad() calls\n",
    "- Calling of model.eval(), enabling/disabling grads during evaluation\n",
    "- Checkpoint Saving and Loading\n",
    "- Tensorboard (see loggers options)\n",
    "- Multi-GPU support\n",
    "- TPU\n",
    "- 16-bit precision AMP support\n",
    "\n",
    "## 5: Use the model\n",
    "\n",
    "Once you’ve trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c76a1e-9f07-4175-8823-a4535ad809c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[-1.2548e+36,  3.1359e+36,  4.9143e+35],\n",
      "        [        nan,         nan,         nan],\n",
      "        [-2.7249e+36, -3.8274e+35,  4.7455e+35],\n",
      "        [-2.4258e+29, -2.9490e+30, -2.0074e+30]], grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\n",
    "autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# choose your trained nn.Module\n",
    "encoder = autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = Tensor(4, 28 * 28)\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bc9a6-7ef1-4199-a196-5b67d44f86e0",
   "metadata": {},
   "source": [
    "## 6: Visualize training\n",
    "\n",
    "If you have tensorboard installed, you can use it for visualizing experiments. TENSORBOARD is niet geinstalleerd!!\n",
    "\n",
    "Run this on your commandline and open your browser to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa161d1-5531-484d-91c4-dbd70af81f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
